{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 03: Online Inference</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\">In this last notebook you will use your deployment for online inference.  </span>\n",
    "\n",
    "## **üóíÔ∏è This notebook is divided into the following sections:** \n",
    "1. **Deployment Retrieval**: Retrieve your deployment from the model registry.\n",
    "2. **Prediction using deployment**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Multiple projects found. \n",
      "\n",
      "\t (1) GraphEmbeddingsDemo\n",
      "\t (2) rixdemo\n",
      "\t (3) BeerVolumePrediction\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/643220\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üóÑ Model Registry</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "# Get the Model Registry\n",
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the \"aml_model\" from the model registry\n",
    "model = mr.get_model(\n",
    "    name=\"aml_model\", \n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>‚öôÔ∏è Fetch Deployment</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1c164e8c704d518edb8cd6949905e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making predictions by using `.predict()`\n"
     ]
    }
   ],
   "source": [
    "# Access the Model Serving\n",
    "ms = project.get_model_serving()\n",
    "\n",
    "# Specify the deployment name\n",
    "deployment_name = \"amlmodeldeployment\"\n",
    "\n",
    "# Get the deployment with the specified name\n",
    "deployment = ms.get_deployment(deployment_name)\n",
    "\n",
    "# Start the deployment and wait for it to be in a running state for up to 300 seconds\n",
    "deployment.start(await_running=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>üîÆ Predicting using deployment</span>\n",
    "\n",
    "\n",
    "Finally you can start making predictions with your model!\n",
    "\n",
    "Send inference requests to the deployed model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data using the input example from the model\n",
    "data = {\n",
    "    \"inputs\": model.input_example,\n",
    "}\n",
    "\n",
    "# Make predictions using the deployed model\n",
    "predictions = deployment.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outputs': 0.255109459}\n"
     ]
    }
   ],
   "source": [
    "# Inspect predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deployment.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anomaly score for node_id  0016359b  :  0.245581672\n",
      " anomaly score for node_id  0054a022  :  0.226151928\n",
      " anomaly score for node_id  00d6b609  :  0.28965959\n",
      " anomaly score for node_id  00e14860  :  0.255449116\n",
      " anomaly score for node_id  014ed5cb  :  0.161434174\n",
      " anomaly score for node_id  01ce3306  :  0.133675143\n",
      " anomaly score for node_id  01fa1d01  :  0.267040074\n",
      " anomaly score for node_id  04b23f4b  :  0.154040545\n"
     ]
    }
   ],
   "source": [
    "# Now lets test feature vectors from online store\n",
    "ids_to_score = [\n",
    "    \"0016359b\", \n",
    "    \"0054a022\", \n",
    "    \"00d6b609\", \n",
    "    \"00e14860\", \n",
    "    \"014ed5cb\", \n",
    "    \"01ce3306\", \n",
    "    \"01fa1d01\", \n",
    "    \"04b23f4b\",\n",
    "]\n",
    "\n",
    "for node_id in ids_to_score:\n",
    "    data = {\"inputs\": [node_id]}\n",
    "    print(\" anomaly score for node_id \", node_id, \" : \",   deployment.predict(data)[\"outputs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For trouble shooting one can use `get_logs` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deployment.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Deployment\n",
    "To stop the deployment you simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üéÅ Wrapping things up </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module you perforemed feature engineering, created feature view and traning dataset, trained advesarial anomaly detection model and depoyed it in production. To setup this pipeline in your enterprise settings contuct us.\n",
    "\n",
    "<img src=\"images/contuct_us.png\" width=\"400px\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
